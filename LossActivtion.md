# Loss function
```
In the chain rule, the derivative of a **composite function** is equal to the product of the derivatives of inner and outer functions.
```
In context of **Deep learning**, the inner function is the **Activation function** while outer function is the **Loss/Cost function**.

In BackPropogation, you have to calculate the gradient of  the cost function which inturns calculates the gradient/derivative
Of ACTIVATION FUNCTION.

<img width="375" alt="image" src="https://github.com/netgvarun2012/portfolio/assets/93938450/be17e92a-2049-45bb-9fba-c241488c1182">

<img width="387" alt="image" src="https://github.com/netgvarun2012/portfolio/assets/93938450/a683c791-15fd-497a-a80b-d59d7e4c37a0">


